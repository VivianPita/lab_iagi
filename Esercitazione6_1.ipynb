{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianPita/lab_iagi/blob/main/Esercitazione6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXhpWIh93nQ6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sapienza-AI-Lab/esercitazione6-22-23/blob/main/Exercise1.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3ZbIw1F3nQ9"
      },
      "source": [
        "## Logistic Regression Implementation\n",
        "In questo esercizio implementeremo la regressione logistica da zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIaOXR9N3nQ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize as opt\n",
        "from sklearn import linear_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hvFdE8K3nQ-"
      },
      "source": [
        "### Esercizio 1 - Sigmoid Function\n",
        "Iniziate implementando la funzione sigmoide, che è definita come:\n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "Nel nostro caso $z = \\theta^Tx$.\n",
        "\n",
        "Non usate cicli for, ma usate le funzioni di numpy per sfruttare il calcolo vettoriale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vu0-h3O3nQ_"
      },
      "outputs": [],
      "source": [
        "# Sigmoid Function\n",
        "def sigmoid(z):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK3D0Rqy3nQ_"
      },
      "source": [
        "### Esercizio 2 - Logistic Regression Cost Function\n",
        "Implementate la funzione di costo per la regressione logistica, che è definita come:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(h_\\theta(x^{(i)})) + (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))$$\n",
        "\n",
        "Usate la funzione sigmoide che avete implementato precedentemente e continuate a sfruttare la vettorizzazione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eem-Gzzv3nQ_"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Cost Function\n",
        "def logistic_cost(W, X, Y):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6agrl6o3nRA"
      },
      "source": [
        "### Esercizio 3 - Gradient Function (single step)\n",
        "\n",
        "Ora implementate la funzione che calcola il gradiente della funzione di costo. Il gradiente è definito come:\n",
        "\n",
        "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvsANO6C3nRB"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Cost Gradient\n",
        "def cost_gradient(W, X, Y):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfYxHcxV3nRB"
      },
      "source": [
        "### Esercizio 4 - Prediction Function\n",
        "\n",
        "Implementate la funzione che calcola la predizione. La predizione è definita come:\n",
        "\n",
        "$$h_\\theta(x) = \\begin{cases} 1 & \\text{se } g(W^Tx) \\geq 0.5 \\\\ 0 & \\text{se } g(W^Tx) < 0.5 \\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OGs5DyY3nRB"
      },
      "outputs": [],
      "source": [
        "# Predict Function\n",
        "def predict(W, X):\n",
        "    raise NotImplementedError(\"You need to implement this function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1dEXEpC3nRB"
      },
      "source": [
        "## Admission Dataset\n",
        "Usiamo l'Admission Dataset per testare le funzioni che abbiamo implementato. Il dataset contiene i risultati di due esami e la decisione di ammissione dei candidati in un'università."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay0GCRJH3nRC"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "path = 'data/exercise1_data.txt'\n",
        "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrGlDGf3nRC"
      },
      "outputs": [],
      "source": [
        "# Visualize data\n",
        "positive = data[data['Admitted'].isin([1])]\n",
        "negative = data[data['Admitted'].isin([0])]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
        "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Exam 1 Score')\n",
        "ax.set_ylabel('Exam 2 Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDNqlj9J3nRC"
      },
      "outputs": [],
      "source": [
        "# Set up input and output matrices\n",
        "X = data[['Exam 1', 'Exam 2']].values\n",
        "m, n = X.shape\n",
        "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
        "n += 1\n",
        "Y = np.array(data[['Admitted']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p90ww1XV3nRC"
      },
      "outputs": [],
      "source": [
        "# Test sigmoid function\n",
        "z = np.linspace(-10, 10, 100)\n",
        "out = sigmoid(z)\n",
        "plt.figure()\n",
        "plt.plot(z, out)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfB-F0jR3nRC"
      },
      "outputs": [],
      "source": [
        "# Test logistic cost function\n",
        "W = np.matrix(np.ones((3, 1))*0.1)\n",
        "print('Test cost function: ', logistic_cost(W, X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D56ncW733nRC"
      },
      "outputs": [],
      "source": [
        "# Test logistic regression cost gradient\n",
        "result = opt.fmin_tnc(func=logistic_cost, x0=W, fprime=cost_gradient, args=(X, Y))\n",
        "print('Logistic cost after optimization: ', logistic_cost(result[0], X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmfYorVT3nRD"
      },
      "outputs": [],
      "source": [
        "# Predict with computed weights\n",
        "Y_hat = predict(result[0], X)\n",
        "accuracy1 = 1.0/m * np.sum(Y_hat == Y.reshape((m,)))\n",
        "\n",
        "print(\"Accuracy with our implementation: \", accuracy1)\n",
        "\n",
        "# Test with sklearn\n",
        "logreg = linear_model.LogisticRegression(penalty=None)\n",
        "logreg.fit(X, Y.reshape((m,)).T)\n",
        "result2 = logreg.predict(X)\n",
        "accuracy2 = np.sum(result2 == Y.reshape((m,))) / m\n",
        "print(\"Accuracy with sklearn: \", accuracy2)\n",
        "\n",
        "# The two accuracies should be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZWSsRRg3nRD"
      },
      "outputs": [],
      "source": [
        "# Visualize decision boundary\n",
        "x1_min, x1_max = X[:, 1].min(), X[:, 1].max(),\n",
        "x2_min, x2_max = X[:, 2].min(), X[:, 2].max(),\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
        "h = sigmoid(np.c_[np.ones((xx1.ravel().shape[0], 1)), xx1.ravel(), xx2.ravel()].dot(result[0]))\n",
        "h = h.reshape(xx1.shape)\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.contour(xx1, xx2, h, [0.5], linewidths=1, colors='g')\n",
        "ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
        "ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
        "ax.legend()\n",
        "ax.set_xlabel('Exam 1 Score')\n",
        "ax.set_ylabel('Exam 2 Score')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyTorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}